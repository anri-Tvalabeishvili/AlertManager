{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235d61cb9e8756ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"Raw Data/ScoringLoans.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6bd4434babe3b1",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "class LocalValidator:\n",
    "\n",
    "    def __init__(self, store=False, history=False, united=True, identifier = None ,path=\"./validation_logs\", file_type=\"pkl\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            store (bool): Whether to store validation results.\n",
    "            history (bool): Whether to store logs with historical data.\n",
    "            united (bool): Whether to store all validations in one file or separately.\n",
    "            path (str): Directory path where logs will be stored.\n",
    "            file_type (str): The file format for storing validation results. Options are 'csv', 'xlsx', 'pkl', 'txt'.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If any of the input arguments are not of the expected type.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Initialize attributes based on user input\n",
    "        self.store = store  # Determines whether to store validation results\n",
    "        self.united = united  # Determines whether to store all validations in one file\n",
    "        self.history = history  # Determines whether to store logs with historical data\n",
    "        self.file_type = file_type.lower()  # File type for storing validation results\n",
    "        self.identifier = identifier  # Determines whether to store logs with historical data\n",
    "\n",
    "        # Set the path for storing logs, including daily subdirectories if history is True\n",
    "        if history:\n",
    "            self.path = os.path.join(path, f\"{datetime.now().strftime('%Y-%m-%d')}\")\n",
    "        else:\n",
    "            self.path = path\n",
    "\n",
    "        # Initialize an empty DataFrame for storing all validation results if united is True\n",
    "        self.all_validations_df = pd.DataFrame()\n",
    "\n",
    "        # Validate the types of the input arguments\n",
    "        if not isinstance(store, bool):\n",
    "            raise TypeError(\"The 'store' argument must be a boolean.\")\n",
    "        if not isinstance(united, bool):\n",
    "            raise TypeError(\"The 'united' argument must be a boolean.\")\n",
    "        if not isinstance(history, bool):\n",
    "            raise TypeError(\"The 'history' argument must be a boolean.\")\n",
    "        if not isinstance(file_type, str):\n",
    "            raise TypeError(\"The 'file_type' argument must be a string.\")\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "\n",
    "    def range_check(self, *, column: str, borders: list, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to validate that the values in a specified column fall within given ranges.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column in the DataFrame to be validated.\n",
    "            borders (list): A list of tuples, each containing two numeric values representing the lower and upper bounds.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "\n",
    "        Returns:\n",
    "            function: A wrapped function with the validation applied.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate input types\n",
    "        if not isinstance(column, str):\n",
    "            raise TypeError(\"The 'column' argument must be a string.\")\n",
    "        if not isinstance(borders, list) or not all(isinstance(i, tuple) and len(i) == 2 for i in borders):\n",
    "            raise TypeError(\"The 'borders' argument must be a list of tuples with two numeric values.\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Check if the specified column exists in the DataFrame\n",
    "                if column not in df.columns:\n",
    "                    raise TypeError(f\"Error: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "                # Initialize a boolean Series to track whether values are within any of the specified ranges\n",
    "                in_range_mask = pd.Series([False] * len(df))\n",
    "\n",
    "                # Iterate over the list of borders and update the mask for values within the range\n",
    "                for bottom, top in borders:\n",
    "                    in_range_mask |= df[column].between(bottom, top)\n",
    "\n",
    "                # Identify rows where values are out of bounds\n",
    "                out_of_bounds = df.loc[~in_range_mask].copy()\n",
    "\n",
    "                # Save the out-of-bounds rows if any exist and storing is enabled\n",
    "                if not out_of_bounds.empty and self.store:\n",
    "                    self.save(out_of_bounds, name)\n",
    "\n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    def value_check(self, *, column: str, allowed: list = None, not_allowed: list = None, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to validate that the values in a specified column are either allowed or not allowed.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column in the DataFrame to be validated.\n",
    "            allowed (list, optional): A list of allowed values for the column.\n",
    "            not_allowed (list, optional): A list of not allowed values for the column.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "\n",
    "        Returns:\n",
    "            function: A wrapped function with the validation applied.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate input types\n",
    "        if not isinstance(column, str):\n",
    "            raise TypeError(\"The 'column' argument must be a string.\")\n",
    "        if allowed is not None and not isinstance(allowed, list):\n",
    "            raise TypeError(\"The 'allowed' argument must be a list.\")\n",
    "        if not_allowed is not None and not isinstance(not_allowed, list):\n",
    "            raise TypeError(\"The 'not_allowed' argument must be a list.\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Check if the specified column exists in the DataFrame\n",
    "                if column not in df.columns:\n",
    "                    raise TypeError(f\"Error: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "                # Initialize an empty DataFrame to store invalid rows\n",
    "                invalid_rows = pd.DataFrame()\n",
    "\n",
    "                # Validate against the allowed list, if provided\n",
    "                if allowed is not None:\n",
    "                    invalid_rows_allowed = df[~df[column].isin(allowed)]\n",
    "                    invalid_rows = pd.concat([invalid_rows, invalid_rows_allowed])\n",
    "\n",
    "                # Validate against the not allowed list, if provided\n",
    "                if not_allowed is not None:\n",
    "                    invalid_rows_not_allowed = df[df[column].isin(not_allowed)]\n",
    "                    invalid_rows = pd.concat([invalid_rows, invalid_rows_not_allowed])\n",
    "\n",
    "                # Save the invalid rows if any exist and storing is enabled\n",
    "                if not invalid_rows.empty and self.store:\n",
    "                    self.save(invalid_rows, name)\n",
    "\n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    def custom_check(self, *, custom_logic, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to apply custom validation logic on a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            custom_logic (str or callable): The custom logic for validation, can be a query string or a function.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "\n",
    "        Returns:\n",
    "            function: A wrapped function with the custom validation applied.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "            ValueError: If the custom logic string or function fails to execute.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate input types\n",
    "        if not (isinstance(custom_logic, str) or callable(custom_logic)):\n",
    "            raise TypeError(\"The 'custom_logic' argument must be a string or a callable (function).\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Apply custom logic if it's a string (query)\n",
    "                if isinstance(custom_logic, str):\n",
    "                    try:\n",
    "                        invalid_rows = df.query(custom_logic)\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error in custom logic: {str(e)}\")\n",
    "\n",
    "                # Apply custom logic if it's a callable (function)\n",
    "                elif callable(custom_logic):\n",
    "                    try:\n",
    "                        invalid_rows = custom_logic(df)\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error in custom function: {str(e)}\")\n",
    "\n",
    "                    # Convert Series result to DataFrame for consistency\n",
    "                    if isinstance(invalid_rows, pd.Series):\n",
    "                        invalid_rows = df.loc[invalid_rows].copy()\n",
    "                    elif not isinstance(invalid_rows, pd.DataFrame):\n",
    "                        raise TypeError(\"The custom function must return a pandas Series or DataFrame.\")\n",
    "\n",
    "                # Save the invalid rows if any exist and storing is enabled\n",
    "                if not invalid_rows.empty and self.store:\n",
    "                    self.save(invalid_rows, name)\n",
    "\n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        return decorator\n",
    "\n",
    "    def statistical(self, *, column: str, name: str, sensitivity=\"medium\", data_type=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to apply statistical outlier detection on a DataFrame column.\n",
    "        Uses z-score for continuous data and frequency-based detection for discrete data.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column in the DataFrame to be validated.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "            sensitivity (str): The sensitivity level of the validation. Options are 'sensitive', 'medium', 'insensitive'.\n",
    "            data_type (str, optional): Specify 'continuous' or 'discrete'. If None, the type will be inferred.\n",
    "\n",
    "        Returns:\n",
    "            function: A wrapped function with the statistical validation applied.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "            ValueError: If an invalid value is provided for 'sensitivity' or 'data_type'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate input types\n",
    "        if not isinstance(column, str):\n",
    "            raise TypeError(\"The 'column' argument must be a string.\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "        if not isinstance(sensitivity, str):\n",
    "            raise TypeError(\"The 'sensitivity' argument must be a string.\")\n",
    "        if sensitivity.lower() not in ['sensitive', 'medium', 'insensitive']:\n",
    "            raise ValueError(\"The 'sensitivity' argument must be one of 'sensitive', 'medium', or 'insensitive'.\")\n",
    "        if data_type is not None and data_type.lower() not in ['continuous', 'discrete']:\n",
    "            raise ValueError(\"The 'data_type' argument must be 'continuous', 'discrete', or None.\")\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Check if the specified column exists in the DataFrame\n",
    "                if column not in df.columns:\n",
    "                    raise TypeError(f\"Error: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "                # Infer data type if not provided\n",
    "                if data_type is None:\n",
    "                    num_unique_values = df[column].nunique()\n",
    "                    total_values = len(df[column])\n",
    "                    unique_ratio = num_unique_values / total_values\n",
    "\n",
    "                    # Heuristic: If the number of unique values is less than 5% of total, treat as discrete\n",
    "                    if unique_ratio < 0.05:\n",
    "                        inferred_type = 'discrete'\n",
    "                    else:\n",
    "                        inferred_type = 'continuous'\n",
    "                else:\n",
    "                    inferred_type = data_type.lower()\n",
    "\n",
    "\n",
    "                # Initialize an empty DataFrame to store outliers\n",
    "                outliers = pd.DataFrame()\n",
    "\n",
    "                if inferred_type == 'continuous':\n",
    "                    # Ensure the column is numeric\n",
    "                    if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                        raise TypeError(f\"Column '{column}' must be numeric for continuous outlier detection.\")\n",
    "\n",
    "                    # Select thresholds based on 'sensitivity'\n",
    "                    if sensitivity.lower() == 'sensitive':\n",
    "                        z_score_threshold = 2.0\n",
    "                    elif sensitivity.lower() == 'medium':\n",
    "                        z_score_threshold = 3.0\n",
    "                    elif sensitivity.lower() == 'insensitive':\n",
    "                        z_score_threshold = 4.0\n",
    "\n",
    "                    # Data is continuous, use z-score method\n",
    "                    mean = df[column].mean()\n",
    "                    std_dev = df[column].std()\n",
    "                    z_scores = np.abs((df[column] - mean) / std_dev)\n",
    "\n",
    "                    # Identify outliers using z-score method\n",
    "                    outliers = df[z_scores > z_score_threshold]\n",
    "\n",
    "                elif inferred_type == 'discrete':\n",
    "                    # Define low frequency threshold percentage based on sensitivity\n",
    "                    if sensitivity.lower() == 'sensitive':\n",
    "                        low_frequency_threshold_percentage = 2\n",
    "                    elif sensitivity.lower() == 'medium':\n",
    "                        low_frequency_threshold_percentage = 1\n",
    "                    elif sensitivity.lower() == 'insensitive':\n",
    "                        low_frequency_threshold_percentage = 0.5\n",
    "\n",
    "                    # Calculate frequency counts\n",
    "                    frequency_counts = df[column].value_counts()\n",
    "                    total_counts = frequency_counts.sum()\n",
    "                    # Determine the threshold for low-frequency values\n",
    "                    low_threshold_value = total_counts * (low_frequency_threshold_percentage / 100.0)\n",
    "                    # Identify values that occur less frequently than the threshold\n",
    "                    outlier_values = frequency_counts[frequency_counts < low_threshold_value].index.tolist()\n",
    "                    # Filter out the rows containing these outlier values\n",
    "                    outliers = df[df[column].isin(outlier_values)]\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid data type specified.\")\n",
    "\n",
    "                # Save the outliers if any exist and storing is enabled\n",
    "                if not outliers.empty and self.store:\n",
    "                    self.save(outliers, name)\n",
    "\n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    def save(self, outliers, name):\n",
    "        \"\"\"\n",
    "        Saves the outliers to a file based on the validator settings.\n",
    "\n",
    "        Args:\n",
    "            outliers (pd.DataFrame): DataFrame containing the outliers.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "        \"\"\"\n",
    "        # Create a copy of the outliers DataFrame to avoid modifying the original\n",
    "        outliers = outliers.copy()\n",
    "\n",
    "        # Add a new column to track the name of the validation that generated the outliers\n",
    "        outliers[\"Validation Name\"] = name\n",
    "\n",
    "        # If united is True, concatenate the outliers with the existing DataFrame of all validations\n",
    "        if self.united:\n",
    "            self.all_validations_df = pd.concat([self.all_validations_df, outliers], ignore_index=True)\n",
    "            # Save the combined DataFrame to a file named 'log' in the specified path\n",
    "            if self.identifier:\n",
    "                self.all_validations_df = self.all_validations_df[[self.identifier,\"Validation Name\"]]\n",
    "            self.save_file(self.all_validations_df, os.path.join(self.path, \"log\"))\n",
    "        else:\n",
    "            # Save the outliers DataFrame to a file named after the validation name\n",
    "            if self.identifier:\n",
    "                outliers = outliers[[self.identifier,\"Validation Name\"]]\n",
    "            self.save_file(outliers, os.path.join(self.path, f\"{name}\"))\n",
    "\n",
    "    def save_file(self, df, file_name):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame to a file in the specified format.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to save.\n",
    "            file_name (str): The path and base name of the file.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the specified file type is not supported.\n",
    "        \"\"\"\n",
    "        # Check the file type and save the DataFrame accordingly\n",
    "        if self.file_type == \"csv\":\n",
    "            df.to_csv(f\"{file_name}.csv\", index=False, encoding='utf-8')\n",
    "        elif self.file_type == \"xlsx\":\n",
    "            df.to_excel(f\"{file_name}.xlsx\", index=False)\n",
    "        elif self.file_type == \"pkl\":\n",
    "            df.to_pickle(f\"{file_name}.pkl\")\n",
    "        elif self.file_type == \"txt\":\n",
    "            with open(f\"{file_name}.txt\", \"w\") as log:\n",
    "                df.to_string(log)\n",
    "                log.write(\"\\n\")\n",
    "        else:\n",
    "            # Raise an error if the file type is not supported\n",
    "            raise ValueError(\"Unsupported file type. Supported types are: 'csv', 'xlsx', 'pkl', 'txt'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d5aed01cd1e7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 15\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;129m@validator\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_check(column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCif\u001B[39m\u001B[38;5;124m'\u001B[39m, not_allowed\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m], name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCif Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;129m@validator\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_check(column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPersonId\u001B[39m\u001B[38;5;124m'\u001B[39m, not_allowed\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m], name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPersonId Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;129m@validator\u001B[39m\u001B[38;5;241m.\u001B[39mrange_check(column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mClientAge\u001B[39m\u001B[38;5;124m'\u001B[39m, borders\u001B[38;5;241m=\u001B[39m[(\u001B[38;5;241m17\u001B[39m, \u001B[38;5;241m100\u001B[39m)], name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAge Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;129m@validator\u001B[39m\u001B[38;5;241m.\u001B[39mstatistical(column\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGenderId\u001B[39m\u001B[38;5;124m'\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenderId Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_data_1\u001B[39m(df):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing data for validation 1...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m process_data_1(df)\n",
      "File \u001B[1;32m~\\Desktop\\Projects\\AlertManager\\AlertManager\\module.py:154\u001B[0m, in \u001B[0;36mLocalValidator.value_check.<locals>.decorator.<locals>.wrapper\u001B[1;34m(df, *args, **kwargs_func)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(invalid_rows, name)\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Execute the wrapped function with the original arguments\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_func)\n",
      "File \u001B[1;32m~\\Desktop\\Projects\\AlertManager\\AlertManager\\module.py:154\u001B[0m, in \u001B[0;36mLocalValidator.value_check.<locals>.decorator.<locals>.wrapper\u001B[1;34m(df, *args, **kwargs_func)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(invalid_rows, name)\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Execute the wrapped function with the original arguments\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_func)\n",
      "File \u001B[1;32m~\\Desktop\\Projects\\AlertManager\\AlertManager\\module.py:98\u001B[0m, in \u001B[0;36mLocalValidator.range_check.<locals>.decorator.<locals>.wrapper\u001B[1;34m(df, *args, **kwargs_func)\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(out_of_bounds, name)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;66;03m# Execute the wrapped function with the original arguments\u001B[39;00m\n\u001B[1;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_func)\n",
      "File \u001B[1;32m~\\Desktop\\Projects\\AlertManager\\AlertManager\\module.py:154\u001B[0m, in \u001B[0;36mLocalValidator.value_check.<locals>.decorator.<locals>.wrapper\u001B[1;34m(df, *args, **kwargs_func)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(invalid_rows, name)\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Execute the wrapped function with the original arguments\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_func)\n",
      "File \u001B[1;32m~\\Desktop\\Projects\\AlertManager\\AlertManager\\module.py:230\u001B[0m, in \u001B[0;36mLocalValidator.statistical.<locals>.decorator.<locals>.wrapper\u001B[1;34m(df, *args, **kwargs_func)\u001B[0m\n\u001B[0;32m    228\u001B[0m mean \u001B[38;5;241m=\u001B[39m df[column]\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m    229\u001B[0m std_dev \u001B[38;5;241m=\u001B[39m df[column]\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m--> 230\u001B[0m z_scores \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs((df[column] \u001B[38;5;241m-\u001B[39m mean) \u001B[38;5;241m/\u001B[39m std_dev)\n\u001B[0;32m    232\u001B[0m \u001B[38;5;66;03m# Identify outliers using z-score method\u001B[39;00m\n\u001B[0;32m    233\u001B[0m outliers \u001B[38;5;241m=\u001B[39m df[z_scores \u001B[38;5;241m>\u001B[39m z_score_threshold]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage of the LocalValidator class.\n",
    "validator = LocalValidator(store=True, history=True, united=True,identifier=\"AppId\", file_type=\"csv\")\n",
    "\n",
    "\n",
    "@validator.value_check(column='Cif', not_allowed=[0], name=\"Cif Validation\")\n",
    "@validator.value_check(column='PersonId', not_allowed=[0], name=\"PersonId Validation\")\n",
    "@validator.range_check(column='ClientAge', borders=[(17, 100)], name=\"Age Validation\")\n",
    "@validator.value_check(column='GenderId', allowed=[9,10], name=\"Gender Validation\")\n",
    "@validator.statistical(column='LoanPeriod', name=\"LoanPeriod Validation\", data_type=\"continuous\")\n",
    "@validator.statistical(column='GenderId', name=\"GenderId Validation\")\n",
    "def process_data_1(df):\n",
    "    print(\"Processing data for validation 1...\")\n",
    "\n",
    "\n",
    "process_data_1(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T17:12:28.706227Z",
     "start_time": "2024-09-28T17:12:27.500906Z"
    }
   },
   "id": "af6e660de1bd7a22",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Example usage of the LocalValidator class.\n",
    "validator = LocalValidator(store=True, history=True, united=True, file_type=\"csv\")\n",
    "\n",
    "# Define custom validation logic as a string\n",
    "custom_logic_str = \"salary == 200000 | name == 'Alice'\"\n",
    "\n",
    "# Define custom validation logic as a function\n",
    "def custom_logic_func(dfss):\n",
    "    result = dfss[(dfss['salary'] == 200000) | (dfss['name'] == 'Alice')]\n",
    "    return result\n",
    "\n",
    "@validator.range_check(column='age', borders=[(18, 30), (50, np.inf)], name=\"Val1\")\n",
    "@validator.value_check(column='name', allowed=['Alice', 'Bob', 'Charlie'], name=\"Val2\")\n",
    "@validator.value_check(column='name', not_allowed=['David'], name=\"Val3\")\n",
    "@validator.custom_check(custom_logic=custom_logic_str, name=\"CustomCheckStr\")\n",
    "@validator.custom_check(custom_logic=custom_logic_func, name=\"CustomCheckFunc\")\n",
    "def process_data_1(df):\n",
    "    print(\"Processing data for validation 1...\")\n",
    "\n",
    "# Apply the decorators and process a DataFrame (df must be defined earlier in the code).\n",
    "process_data_1(df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a2a9c3c8f26332",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# დავამატოთ დათაფრეიმში აუთლეიერების პონის ფუნქციონალი, რომელიც გადაცემული სვეტის ან სვეტების მიხედვით იპოვის აუთლეიერს და გამოუტანს იუზერს, თუ სვეტს არ დაუკონკრეტებ მაშინ დათაფრეიმის თითოეულ სვეტში მოგიძებნის აუთლეიერებს. \n",
    "# ზემოთ დაწერილი ფუნქციონალი უნდა დაიწეროს MS SQL, PostgreSQL, MySQL"
   ],
   "id": "fb2ae68989d45f7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
